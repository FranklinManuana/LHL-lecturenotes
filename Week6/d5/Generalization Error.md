## Generalization Error

Generalization error in machine learning is the difference between the performance of a trained model on the training data and its performance on new, unseen data. In simpler terms, it measures how well a machine learning model is able to make accurate predictions on new data that it hasn't seen before. The goal of machine learning is to minimize the generalization error, so that the model can make accurate predictions on new data

In the last reading, we saw that the main goal of supervised learning is to find a magical function **`h`**, where `h(x) = y` for all examples. To evaluate the effectiveness of a machine learning algorithm, we compare the predicted values (referred to as y_prediction) generated by the algorithm with the actual labels (y_true) in the dataset. The difference between these two variables can serve as a useful metric for validating the accuracy of the machine learning algorithm.  
This is typically achieved through a function known as the "loss function". The loss function measures the difference between the predicted and actual values and provides feedback to the model on how well it is performing.

Let’s take a look at a simple loss function in regression tasks called RMSE (Root Mean Square Error).

RMSE (Root Mean Squared Error) is a measure of the difference between the predicted and actual values of a target variable. It is commonly used in machine learning to evaluate the accuracy of a model in predicting numerical values. RMSE is calculated by taking the square root of the average of the squared differences between the predicted and actual values. In simpler terms, it provides a single number that represents how far off the predicted values are from the actual values, with a lower RMSE indicating a more accurate model.

Instruction

Watch the video below to learn more about the `loss` function. 

The objective of the machine learning task is then to find a model (h) that minimizes this loss function. The goal is to create a model that produces predictions that are as close as possible to the actual labels in the dataset. Once we find a function that minimizes the loss function, we have a supervised machine learning model that is ready to make predictions on new data.

## What can go wrong?

There are three main things that can go wrong during the training of ML model which contribute to the overall _generalization error_:

- Approximation Error
- Estimation Error
- Optimization Error

Let's take a look at the picture below:

![Function for our training model, described on this page below.](https://i.imgur.com/g02U9sL.png)

The **`h`** on the left, marked `Would like to have`, is the theoretical solution that is 100% correct (in theory). The **`h`** on the right, marked `We actually have`, is the function we can get by training our model. There are 3 types of error standing in the way of perfect solution:

### Approximation Error

Approximation error is the difference between the true or actual value and the estimated value obtained by an approximation method or model. In the context of machine learning, approximation error is the difference between the actual function that maps input variables to output variables and the estimated function produced by a machine learning model. In simpler terms, it is the difference between the ideal solution and the best solution we can obtain using an approximation method. The goal of machine learning is to minimize both approximation error and generalization error, so that the model can accurately predict outcomes on new data.

### Estimation Error

Estimation error is the difference between the true distribution of data and the estimated distribution of data obtained by a machine learning model. In simpler terms, it is the error or uncertainty that arises when we use sample data to estimate parameters or make predictions about new data. The goal of machine learning is to minimize both estimation error and generalization error, so that the model can accurately predict outcomes on new data.

In machine learning, estimation error can be divided into two categories: bias and variance. 1. Bias (Underfitting): Bias is the difference between the average prediction of a model and the true value. If a model has high bias, it means that it is not flexible enough to capture the true relationship between the input and output variables. This can result in underfitting, where the model is too simple and cannot accurately capture the complexity of the data. 2. Variance (Overfitting): Variance is the variability of a model's predictions for a given input. If a model has high variance, it means that it is too sensitive to the noise or randomness in the training data. This can result in overfitting, where the model is too complex and fits too closely to the training data, but fails to generalize well to new data.

The trade-off between bias and variance is an important consideration in machine learning. A good model should strike a balance between the two, by being flexible enough to capture the underlying patterns in the data, but not so complex that it overfits to the training data and fails to generalize well.

### Optimization Error

_The optimization error_ occurs when we have a loss function which is too complex, and as a result we don't find the optimal solution. A huge amount of observations can increase the optimization error as well.

## Conclusion

We have learned what kind of errors affect our predictions. It is crucial to understand them to be able to "tune" our models in case they are not perfect on the first try. We need to understand that if we choose the wrong `H` it doesn't matter if we add more data and vice versa.

Now, we can move on to the first supervised learning model we will get to know – _linear regression_.